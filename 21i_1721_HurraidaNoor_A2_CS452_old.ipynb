{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTj4v-g6t3pF",
        "outputId": "220fd9d3-550f-4e5b-d334-6ef378d04a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Check GPU and set seeds for reproducibility\n",
        "import os, random, time\n",
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"No GPU available\")\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "import numpy as np\n",
        "np.random.seed(SEED)s\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pIbm83JJviNc"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip --quiet\n",
        "!pip install torch torchvision torchaudio --quiet\n",
        "!pip install pandas scikit-learn matplotlib seaborn tqdm --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20rn9uw11ABs",
        "outputId": "d839c327-b88a-4221-85a9-b30355167fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  archive.zip\n",
            "  inflating: dataset/absence-of-certain-changes-or-events.csv  \n",
            "  inflating: dataset/absence-of-certain-changes.csv  \n",
            "  inflating: dataset/acceleration.csv  \n",
            "  inflating: dataset/access-to-information.csv  \n",
            "  inflating: dataset/access.csv      \n",
            "  inflating: dataset/accounting-terms.csv  \n",
            "  inflating: dataset/additional-agreements.csv  \n",
            "  inflating: dataset/additional-documents.csv  \n",
            "  inflating: dataset/adjustments.csv  \n",
            "  inflating: dataset/affirmative-covenants.csv  \n",
            "  inflating: dataset/agreement.csv   \n",
            "  inflating: dataset/agreements.csv  \n",
            "  inflating: dataset/amendment-and-waiver.csv  \n",
            "  inflating: dataset/amendment-waiver.csv  \n",
            "  inflating: dataset/amendment.csv   \n",
            "  inflating: dataset/amendments-and-waivers.csv  \n",
            "  inflating: dataset/amendments-etc.csv  \n",
            "  inflating: dataset/amendments-waivers.csv  \n",
            "  inflating: dataset/amendments.csv  \n",
            "  inflating: dataset/applicable-law.csv  \n",
            "  inflating: dataset/application-of-proceeds.csv  \n",
            "  inflating: dataset/appointment.csv  \n",
            "  inflating: dataset/approvals.csv   \n",
            "  inflating: dataset/arbitration.csv  \n",
            "  inflating: dataset/assignability.csv  \n",
            "  inflating: dataset/assignment.csv  \n",
            "  inflating: dataset/assignments.csv  \n",
            "  inflating: dataset/attorneys-fees.csv  \n",
            "  inflating: dataset/authority.csv   \n",
            "  inflating: dataset/authorization.csv  \n",
            "  inflating: dataset/auto_renewal.csv  \n",
            "  inflating: dataset/background.csv  \n",
            "  inflating: dataset/bank-accounts.csv  \n",
            "  inflating: dataset/bankruptcy.csv  \n",
            "  inflating: dataset/base-salary.csv  \n",
            "  inflating: dataset/benefits.csv    \n",
            "  inflating: dataset/bereavement-leave.csv  \n",
            "  inflating: dataset/binding-effect.csv  \n",
            "  inflating: dataset/bonus.csv       \n",
            "  inflating: dataset/books-and-records.csv  \n",
            "  inflating: dataset/brokerage.csv   \n",
            "  inflating: dataset/brokers.csv     \n",
            "  inflating: dataset/cancellation.csv  \n",
            "  inflating: dataset/capitalization.csv  \n",
            "  inflating: dataset/capitalized-terms.csv  \n",
            "  inflating: dataset/captions.csv    \n",
            "  inflating: dataset/cause.csv       \n",
            "  inflating: dataset/certain-defined-terms.csv  \n",
            "  inflating: dataset/certain-definitions.csv  \n",
            "  inflating: dataset/certificates.csv  \n",
            "  inflating: dataset/change-in-control.csv  \n",
            "  inflating: dataset/change-of-control.csv  \n",
            "  inflating: dataset/choice-of-law.csv  \n",
            "  inflating: dataset/closing-date.csv  \n",
            "  inflating: dataset/closing.csv     \n",
            "  inflating: dataset/collateral.csv  \n",
            "  inflating: dataset/compensation-and-benefits.csv  \n",
            "  inflating: dataset/compensation.csv  \n",
            "  inflating: dataset/complete-agreement.csv  \n",
            "  inflating: dataset/compliance-certificate.csv  \n",
            "  inflating: dataset/compliance-with-law.csv  \n",
            "  inflating: dataset/compliance-with-laws.csv  \n",
            "  inflating: dataset/compliance.csv  \n",
            "  inflating: dataset/condemnation.csv  \n",
            "  inflating: dataset/conditions-precedent.csv  \n",
            "  inflating: dataset/conditions-to-effectiveness.csv  \n",
            "  inflating: dataset/conditions.csv  \n",
            "  inflating: dataset/conduct-of-business.csv  \n",
            "  inflating: dataset/confidential-information.csv  \n",
            "  inflating: dataset/confidentiality.csv  \n",
            "  inflating: dataset/conflict-of-interest.csv  \n",
            "  inflating: dataset/consent-to-jurisdiction.csv  \n",
            "  inflating: dataset/consents-and-approvals.csv  \n",
            "  inflating: dataset/consents.csv    \n",
            "  inflating: dataset/consideration.csv  \n",
            "  inflating: dataset/construction.csv  \n",
            "  inflating: dataset/contracts.csv   \n",
            "  inflating: dataset/contribution.csv  \n",
            "  inflating: dataset/cooperation.csv  \n",
            "  inflating: dataset/corporate-existence.csv  \n",
            "  inflating: dataset/costs-and-expenses.csv  \n",
            "  inflating: dataset/costs.csv       \n",
            "  inflating: dataset/counterparts.csv  \n",
            "  inflating: dataset/covenants-of-the-company.csv  \n",
            "  inflating: dataset/covenants.csv   \n",
            "  inflating: dataset/currency.csv    \n",
            "  inflating: dataset/death.csv       \n",
            "  inflating: dataset/default.csv     \n",
            "  inflating: dataset/defaults-and-remedies.csv  \n",
            "  inflating: dataset/defaults.csv    \n",
            "  inflating: dataset/defined-terms.csv  \n",
            "  inflating: dataset/definition.csv  \n",
            "  inflating: dataset/definitions-and-interpretation.csv  \n",
            "  inflating: dataset/definitions.csv  \n",
            "  inflating: dataset/delegation-of-duties.csv  \n",
            "  inflating: dataset/delivery.csv    \n",
            "  inflating: dataset/descriptive-headings.csv  \n",
            "  inflating: dataset/disability.csv  \n",
            "  inflating: dataset/disclaimer.csv  \n",
            "  inflating: dataset/disclosure.csv  \n",
            "  inflating: dataset/dispute-resolution.csv  \n",
            "  inflating: dataset/disputes.csv    \n",
            "  inflating: dataset/dissolution.csv  \n",
            "  inflating: dataset/distributions.csv  \n",
            "  inflating: dataset/dividends.csv   \n",
            "  inflating: dataset/documents.csv   \n",
            "  inflating: dataset/duration-and-termination.csv  \n",
            "  inflating: dataset/duration-of-agreement.csv  \n",
            "  inflating: dataset/duration.csv    \n",
            "  inflating: dataset/duties.csv      \n",
            "  inflating: dataset/effect-of-headings.csv  \n",
            "  inflating: dataset/effect-of-termination.csv  \n",
            "  inflating: dataset/effective-date.csv  \n",
            "  inflating: dataset/effectiveness.csv  \n",
            "  inflating: dataset/employee-benefit-plans.csv  \n",
            "  inflating: dataset/employee-benefits.csv  \n",
            "  inflating: dataset/employees.csv   \n",
            "  inflating: dataset/employment.csv  \n",
            "  inflating: dataset/enforceability.csv  \n",
            "  inflating: dataset/enforcement.csv  \n",
            "  inflating: dataset/entire-agreement-amendment.csv  \n",
            "  inflating: dataset/entire-agreement-amendments.csv  \n",
            "  inflating: dataset/entire-agreement.csv  \n",
            "  inflating: dataset/environmental-laws.csv  \n",
            "  inflating: dataset/environmental-matters.csv  \n",
            "  inflating: dataset/erisa.csv       \n",
            "  inflating: dataset/event-of-default.csv  \n",
            "  inflating: dataset/events-of-default.csv  \n",
            "  inflating: dataset/exceptions.csv  \n",
            "  inflating: dataset/exclusions.csv  \n",
            "  inflating: dataset/exclusivity.csv  \n",
            "  inflating: dataset/execution-in-counterparts.csv  \n",
            "  inflating: dataset/execution.csv   \n",
            "  inflating: dataset/exercise-of-option.csv  \n",
            "  inflating: dataset/exhibits.csv    \n",
            "  inflating: dataset/expenses.csv    \n",
            "  inflating: dataset/fees-and-expenses.csv  \n",
            "  inflating: dataset/fees.csv        \n",
            "  inflating: dataset/fees_royalties.csv  \n",
            "  inflating: dataset/financial-covenants.csv  \n",
            "  inflating: dataset/financial-information.csv  \n",
            "  inflating: dataset/financial-statements.csv  \n",
            "  inflating: dataset/financing.csv   \n",
            "  inflating: dataset/fiscal-year.csv  \n",
            "  inflating: dataset/force-majeure.csv  \n",
            "  inflating: dataset/fractional-shares.csv  \n",
            "  inflating: dataset/full-disclosure.csv  \n",
            "  inflating: dataset/further-assurances.csv  \n",
            "  inflating: dataset/general-provisions.csv  \n",
            "  inflating: dataset/general.csv     \n",
            "  inflating: dataset/generally.csv   \n",
            "  inflating: dataset/good-reason.csv  \n",
            "  inflating: dataset/governing-law-and-jurisdiction.csv  \n",
            "  inflating: dataset/governing-law-jurisdiction.csv  \n",
            "  inflating: dataset/governing-law.csv  \n",
            "  inflating: dataset/grant-of-option.csv  \n",
            "  inflating: dataset/grant-of-security-interest.csv  \n",
            "  inflating: dataset/grievance-procedure.csv  \n",
            "  inflating: dataset/guarantee.csv   \n",
            "  inflating: dataset/guaranty.csv    \n",
            "  inflating: dataset/headings.csv    \n",
            "  inflating: dataset/holidays.csv    \n",
            "  inflating: dataset/illegality.csv  \n",
            "  inflating: dataset/in-witness-whereof.csv  \n",
            "  inflating: dataset/increased-costs.csv  \n",
            "  inflating: dataset/indebtedness.csv  \n",
            "  inflating: dataset/indemnification-and-contribution.csv  \n",
            "  inflating: dataset/indemnification-by-the-company.csv  \n",
            "  inflating: dataset/indemnification.csv  \n",
            "  inflating: dataset/indemnity.csv   \n",
            "  inflating: dataset/independent-contractor.csv  \n",
            "  inflating: dataset/information.csv  \n",
            "  inflating: dataset/injunctive-relief.csv  \n",
            "  inflating: dataset/insolvency.csv  \n",
            "  inflating: dataset/inspection.csv  \n",
            "  inflating: dataset/insurance.csv   \n",
            "  inflating: dataset/integration.csv  \n",
            "  inflating: dataset/intellectual-property-rights.csv  \n",
            "  inflating: dataset/intellectual-property.csv  \n",
            "  inflating: dataset/interest.csv    \n",
            "  inflating: dataset/interpretation.csv  \n",
            "  inflating: dataset/introduction.csv  \n",
            "  inflating: dataset/investment-company-act.csv  \n",
            "  inflating: dataset/investment-company.csv  \n",
            "  inflating: dataset/investments.csv  \n",
            "  inflating: dataset/judgments.csv   \n",
            "  inflating: dataset/jurisdiction.csv  \n",
            "  inflating: dataset/labor-matters.csv  \n",
            "  inflating: dataset/legal-proceedings.csv  \n",
            "  inflating: dataset/legend.csv      \n",
            "  inflating: dataset/legends.csv     \n",
            "  inflating: dataset/letters-of-credit.csv  \n",
            "  inflating: dataset/liabilities.csv  \n",
            "  inflating: dataset/liability.csv   \n",
            "  inflating: dataset/licenses.csv    \n",
            "  inflating: dataset/liens.csv       \n",
            "  inflating: dataset/limitation-of-liability.csv  \n",
            "  inflating: dataset/limitation-on-liability.csv  \n",
            "  inflating: dataset/limitations.csv  \n",
            "  inflating: dataset/limited_liability.csv  \n",
            "  inflating: dataset/listing.csv     \n",
            "  inflating: dataset/litigation.csv  \n",
            "  inflating: dataset/loans.csv       \n",
            "  inflating: dataset/maintenance-of-insurance.csv  \n",
            "  inflating: dataset/maintenance-of-office-or-agency.csv  \n",
            "  inflating: dataset/maintenance-of-properties.csv  \n",
            "  inflating: dataset/management-rights.csv  \n",
            "  inflating: dataset/management.csv  \n",
            "  inflating: dataset/marketing.csv   \n",
            "  inflating: dataset/material-contracts.csv  \n",
            "  inflating: dataset/meetings.csv    \n",
            "  inflating: dataset/merger.csv      \n",
            "  inflating: dataset/method-of-payment.csv  \n",
            "  inflating: dataset/miscellaneous-provisions.csv  \n",
            "  inflating: dataset/miscellaneous.csv  \n",
            "  inflating: dataset/modification-and-waiver.csv  \n",
            "  inflating: dataset/modification.csv  \n",
            "  inflating: dataset/modifications.csv  \n",
            "  inflating: dataset/name.csv        \n",
            "  inflating: dataset/negative-covenants.csv  \n",
            "  inflating: dataset/no-assignment.csv  \n",
            "  inflating: dataset/no-conflict.csv  \n",
            "  inflating: dataset/no-conflicts.csv  \n",
            "  inflating: dataset/no-default.csv  \n",
            "  inflating: dataset/no-material-adverse-change.csv  \n",
            "  inflating: dataset/no-solicitation.csv  \n",
            "  inflating: dataset/no-strict-construction.csv  \n",
            "  inflating: dataset/no-third-party-beneficiaries.csv  \n",
            "  inflating: dataset/no-violation.csv  \n",
            "  inflating: dataset/no-waiver.csv   \n",
            "  inflating: dataset/non-competition.csv  \n",
            "  inflating: dataset/non-discrimination.csv  \n",
            "  inflating: dataset/non-disparagement.csv  \n",
            "  inflating: dataset/non-solicitation.csv  \n",
            "  inflating: dataset/notice-of-default.csv  \n",
            "  inflating: dataset/notice-of-defaults.csv  \n",
            "  inflating: dataset/notice-of-redemption.csv  \n",
            "  inflating: dataset/notice-of-termination.csv  \n",
            "  inflating: dataset/notice.csv      \n",
            "  inflating: dataset/notices-etc.csv  \n",
            "  inflating: dataset/notices.csv     \n",
            "  inflating: dataset/notification.csv  \n",
            "  inflating: dataset/now-therefore.csv  \n",
            "  inflating: dataset/obligations-absolute.csv  \n",
            "  inflating: dataset/officers-certificate.csv  \n",
            "  inflating: dataset/officers.csv    \n",
            "  inflating: dataset/optional_renewal.csv  \n",
            "  inflating: dataset/organization-and-good-standing.csv  \n",
            "  inflating: dataset/organization-and-qualification.csv  \n",
            "  inflating: dataset/organization.csv  \n",
            "  inflating: dataset/other-agreements.csv  \n",
            "  inflating: dataset/other-benefits.csv  \n",
            "  inflating: dataset/other-provisions.csv  \n",
            "  inflating: dataset/other-remedies.csv  \n",
            "  inflating: dataset/other.csv       \n",
            "  inflating: dataset/ownership.csv   \n",
            "  inflating: dataset/parking.csv     \n",
            "  inflating: dataset/partial-invalidity.csv  \n",
            "  inflating: dataset/participations.csv  \n",
            "  inflating: dataset/parties-in-interest.csv  \n",
            "  inflating: dataset/parties.csv     \n",
            "  inflating: dataset/payment-of-expenses.csv  \n",
            "  inflating: dataset/payment-of-obligations.csv  \n",
            "  inflating: dataset/payment-of-taxes.csv  \n",
            "  inflating: dataset/payment-terms.csv  \n",
            "  inflating: dataset/payment.csv     \n",
            "  inflating: dataset/payments.csv    \n",
            "  inflating: dataset/performance.csv  \n",
            "  inflating: dataset/permits.csv     \n",
            "  inflating: dataset/person.csv      \n",
            "  inflating: dataset/persons-deemed-owners.csv  \n",
            "  inflating: dataset/position-and-duties.csv  \n",
            "  inflating: dataset/power-and-authority.csv  \n",
            "  inflating: dataset/power-of-attorney.csv  \n",
            "  inflating: dataset/powers.csv      \n",
            "  inflating: dataset/preamble.csv    \n",
            "  inflating: dataset/procedure.csv   \n",
            "  inflating: dataset/proprietary_rights.csv  \n",
            "  inflating: dataset/public-announcements.csv  \n",
            "  inflating: dataset/publicity.csv   \n",
            "  inflating: dataset/purchase-and-sale.csv  \n",
            "  inflating: dataset/purchase-price.csv  \n",
            "  inflating: dataset/purpose.csv     \n",
            "  inflating: dataset/r-e-c-i-t-a-l-s.csv  \n",
            "  inflating: dataset/real-property.csv  \n",
            "  inflating: dataset/recitals.csv    \n",
            "  inflating: dataset/recognition.csv  \n",
            "  inflating: dataset/records.csv     \n",
            "  inflating: dataset/redemption.csv  \n",
            "  inflating: dataset/registration-expenses.csv  \n",
            "  inflating: dataset/registration-procedures.csv  \n",
            "  inflating: dataset/registration-rights.csv  \n",
            "  inflating: dataset/registration.csv  \n",
            "  inflating: dataset/reimbursement-of-expenses.csv  \n",
            "  inflating: dataset/reimbursement.csv  \n",
            "  inflating: dataset/reinstatement.csv  \n",
            "  inflating: dataset/relationship-of-parties.csv  \n",
            "  inflating: dataset/relationship-of-the-parties.csv  \n",
            "  inflating: dataset/release.csv     \n",
            "  inflating: dataset/reliance.csv    \n",
            "  inflating: dataset/remedies-cumulative.csv  \n",
            "  inflating: dataset/remedies.csv    \n",
            "  inflating: dataset/rent.csv        \n",
            "  inflating: dataset/reporting-requirements.csv  \n",
            "  inflating: dataset/reporting.csv   \n",
            "  inflating: dataset/reports.csv     \n",
            "  inflating: dataset/representations-and-warranties-of-seller.csv  \n",
            "  inflating: dataset/representations-and-warranties-of-the-company.csv  \n",
            "  inflating: dataset/representations-and-warranties.csv  \n",
            "  inflating: dataset/representations-warranties-and-covenants.csv  \n",
            "  inflating: dataset/representations.csv  \n",
            "  inflating: dataset/resignation.csv  \n",
            "  inflating: dataset/restricted-payments.csv  \n",
            "  inflating: dataset/restrictions-on-transfer.csv  \n",
            "  inflating: dataset/restrictions.csv  \n",
            "  inflating: dataset/restrictive-covenants.csv  \n",
            "  inflating: dataset/right-of-setoff.csv  \n",
            "  inflating: dataset/rules-of-construction.csv  \n",
            "  inflating: dataset/salary.csv      \n",
            "  inflating: dataset/scope.csv       \n",
            "  inflating: dataset/section-headings.csv  \n",
            "  inflating: dataset/security-deposit.csv  \n",
            "  inflating: dataset/security-interest.csv  \n",
            "  inflating: dataset/security.csv    \n",
            "  inflating: dataset/seniority.csv   \n",
            "  inflating: dataset/services.csv    \n",
            "  inflating: dataset/set-off.csv     \n",
            "  inflating: dataset/severability-of-provisions.csv  \n",
            "  inflating: dataset/severability.csv  \n",
            "  inflating: dataset/severance.csv   \n",
            "  inflating: dataset/sick-leave.csv  \n",
            "  inflating: dataset/solvency.csv    \n",
            "  inflating: dataset/special-terms-and-conditions-of-trust.csv  \n",
            "  inflating: dataset/specific-performance.csv  \n",
            "  inflating: dataset/standard-of-care.csv  \n",
            "  inflating: dataset/standard-terms-and-conditions-of-trust.csv  \n",
            "  inflating: dataset/stock-options.csv  \n",
            "  inflating: dataset/submission-to-jurisdiction.csv  \n",
            "  inflating: dataset/subordination.csv  \n",
            "  inflating: dataset/subrogation.csv  \n",
            "  inflating: dataset/subsidiaries.csv  \n",
            "  inflating: dataset/successors-and-assigns.csv  \n",
            "  inflating: dataset/successors.csv  \n",
            "  inflating: dataset/support.csv     \n",
            "  inflating: dataset/survival-of-representations-and-warranties.csv  \n",
            "  inflating: dataset/survival.csv    \n",
            "  inflating: dataset/tax-matters.csv  \n",
            "  inflating: dataset/tax-returns.csv  \n",
            "  inflating: dataset/tax-withholding.csv  \n",
            "  inflating: dataset/taxes.csv       \n",
            "  inflating: dataset/term-and-termination.csv  \n",
            "  inflating: dataset/term-of-agreement.csv  \n",
            "  inflating: dataset/term-of-employment.csv  \n",
            "  inflating: dataset/term.csv        \n",
            "  inflating: dataset/termination-for-cause.csv  \n",
            "  inflating: dataset/termination-of-agreement.csv  \n",
            "  inflating: dataset/termination-of-employment.csv  \n",
            "  inflating: dataset/termination-without-cause.csv  \n",
            "  inflating: dataset/termination.csv  \n",
            "  inflating: dataset/terms.csv       \n",
            "  inflating: dataset/the-closing.csv  \n",
            "  inflating: dataset/the-merger.csv  \n",
            "  inflating: dataset/therefore.csv   \n",
            "  inflating: dataset/third-party-beneficiaries.csv  \n",
            "  inflating: dataset/time-of-essence.csv  \n",
            "  inflating: dataset/time-of-the-essence.csv  \n",
            "  inflating: dataset/time.csv        \n",
            "  inflating: dataset/title-to-assets.csv  \n",
            "  inflating: dataset/title.csv       \n",
            "  inflating: dataset/transactions-with-affiliates.csv  \n",
            "  inflating: dataset/transfer.csv    \n",
            "  inflating: dataset/transfers.csv   \n",
            "  inflating: dataset/trustee-may-file-proofs-of-claim.csv  \n",
            "  inflating: dataset/undertaking-for-costs.csv  \n",
            "  inflating: dataset/usa-patriot-act.csv  \n",
            "  inflating: dataset/use-of-proceeds.csv  \n",
            "  inflating: dataset/use.csv         \n",
            "  inflating: dataset/utilities.csv   \n",
            "  inflating: dataset/vacation.csv    \n",
            "  inflating: dataset/validity.csv    \n",
            "  inflating: dataset/vesting.csv     \n",
            "  inflating: dataset/voting-rights.csv  \n",
            "  inflating: dataset/voting.csv      \n",
            "  inflating: dataset/w-i-t-n-e-s-s-e-t-h-whereas.csv  \n",
            "  inflating: dataset/waiver-of-jury-trial.csv  \n",
            "  inflating: dataset/waiver-of-past-defaults.csv  \n",
            "  inflating: dataset/waiver.csv      \n",
            "  inflating: dataset/waivers.csv     \n",
            "  inflating: dataset/warranties.csv  \n",
            "  inflating: dataset/warranty.csv    \n",
            "  inflating: dataset/whereas.csv     \n",
            "  inflating: dataset/withholding-taxes.csv  \n",
            "  inflating: dataset/withholding.csv  \n",
            "  inflating: dataset/witnesseth-that.csv  \n",
            "  inflating: dataset/witnesseth.csv  \n",
            "Number of CSV files found: 395\n",
            "['dataset/organization.csv', 'dataset/independent-contractor.csv', 'dataset/registration-rights.csv', 'dataset/amendments-and-waivers.csv', 'dataset/term-of-employment.csv', 'dataset/indebtedness.csv', 'dataset/transfer.csv', 'dataset/now-therefore.csv', 'dataset/time.csv', 'dataset/r-e-c-i-t-a-l-s.csv']\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Unzip the uploaded archive safely\n",
        "\n",
        "import os\n",
        "\n",
        "# Path to your uploaded zip\n",
        "zip_path = \"archive.zip\"\n",
        "\n",
        "# Make sure the dataset folder exists fresh\n",
        "if os.path.exists(\"dataset\"):\n",
        "    !rm -rf dataset\n",
        "\n",
        "# Unzip archive into 'dataset' folder, overwrite if necessary\n",
        "!unzip -o \"{zip_path}\" -d dataset\n",
        "\n",
        "# List all CSV files to confirm\n",
        "import glob\n",
        "csv_files = glob.glob(\"dataset/**/*.csv\", recursive=True)\n",
        "print(\"Number of CSV files found:\", len(csv_files))\n",
        "print(csv_files[:10])  # show first 10 files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrA1XpS43vwE",
        "outputId": "f1ee59d9-cebc-4cfc-87dd-132e22e97b2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Fix random seed\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2xTjdWX4hwP",
        "outputId": "f68e10e7-f806-4bbf-a7d8-66fa13fc729b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pairs: 8249580\n"
          ]
        }
      ],
      "source": [
        "# Build clause pairs\n",
        "def build_pairs_from_folder(folder_path, max_per_category=None, neg_ratio=1):\n",
        "    files = glob.glob(os.path.join(folder_path, \"**/*.csv\"), recursive=True)\n",
        "    if len(files) == 0:\n",
        "        raise ValueError(f\"No CSV files found in {folder_path}.\")\n",
        "\n",
        "    categories = {}\n",
        "    for f in files:\n",
        "        df = pd.read_csv(f)\n",
        "        if 'clause_text' not in df.columns:\n",
        "            continue\n",
        "        texts = df['clause_text'].dropna().astype(str).tolist()\n",
        "        if max_per_category:\n",
        "            texts = texts[:max_per_category]\n",
        "        cat = os.path.splitext(os.path.basename(f))[0]\n",
        "        categories[cat] = texts\n",
        "\n",
        "    pairs = []\n",
        "    # Positive pairs\n",
        "    for cat, texts in categories.items():\n",
        "        for i in range(len(texts)):\n",
        "            for j in range(i+1, len(texts)):\n",
        "                pairs.append((texts[i], texts[j], 1))\n",
        "    # Negative pairs\n",
        "    cats = list(categories.keys())\n",
        "    for cat_a in cats:\n",
        "        for cat_b in cats:\n",
        "            if cat_a == cat_b:\n",
        "                continue\n",
        "            list_a = categories[cat_a]\n",
        "            list_b = categories[cat_b]\n",
        "            for a in list_a:\n",
        "                for _ in range(neg_ratio):\n",
        "                    b = random.choice(list_b)\n",
        "                    pairs.append((a, b, 0))\n",
        "    random.shuffle(pairs)\n",
        "    return pairs\n",
        "\n",
        "# Build pairs (use max_per_category=50 to speed up in Colab)\n",
        "pairs = build_pairs_from_folder(\"dataset\", max_per_category=50, neg_ratio=1)\n",
        "print(\"Total pairs:\", len(pairs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqoQOl0Z4hy1",
        "outputId": "12253b4a-372d-4bc7-da97-fae8172cc396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 6599664, Val: 824958, Test: 824958\n"
          ]
        }
      ],
      "source": [
        "# Split dataset\n",
        "train_pairs, temp_pairs = train_test_split(pairs, test_size=0.2, random_state=SEED)\n",
        "val_pairs, test_pairs = train_test_split(temp_pairs, test_size=0.5, random_state=SEED)\n",
        "\n",
        "print(f\"Train: {len(train_pairs)}, Val: {len(val_pairs)}, Test: {len(test_pairs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7MTE8Pk4h1g",
        "outputId": "42fc7a8c-845b-4217-9bc2-13ea32f7f3d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 38814\n"
          ]
        }
      ],
      "source": [
        "# Simple tokenization (word-level, limited vocab)\n",
        "from collections import Counter\n",
        "\n",
        "MAX_SEQ_LEN = 50  # limit to reduce memory usage\n",
        "\n",
        "def build_vocab(pairs, min_freq=1):\n",
        "    counter = Counter()\n",
        "    for t1, t2, _ in pairs:\n",
        "        counter.update(t1.lower().split())\n",
        "        counter.update(t2.lower().split())\n",
        "    vocab = {w:i+2 for i, (w, freq) in enumerate(counter.items()) if freq >= min_freq}  # 0=PAD, 1=UNK\n",
        "    vocab['<PAD>'] = 0\n",
        "    vocab['<UNK>'] = 1\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(train_pairs)\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "def encode_text(text, vocab, max_len=MAX_SEQ_LEN):\n",
        "    tokens = [vocab.get(w, vocab['<UNK>']) for w in text.lower().split()]\n",
        "    tokens = tokens[:max_len]\n",
        "    tokens += [vocab['<PAD>']] * (max_len - len(tokens))\n",
        "    return tokens\n",
        "\n",
        "class ClausePairDataset(Dataset):\n",
        "    def __init__(self, pairs, vocab):\n",
        "        self.pairs = pairs\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        t1, t2, label = self.pairs[idx]\n",
        "        t1_enc = torch.tensor(encode_text(t1, self.vocab))\n",
        "        t2_enc = torch.tensor(encode_text(t2, self.vocab))\n",
        "        label = torch.tensor(label, dtype=torch.float)\n",
        "        return t1_enc, t2_enc, label\n",
        "\n",
        "# Dataloaders\n",
        "BATCH_SIZE = 32\n",
        "train_dataset = ClausePairDataset(train_pairs, vocab)\n",
        "val_dataset = ClausePairDataset(val_pairs, vocab)\n",
        "test_dataset = ClausePairDataset(test_pairs, vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQw70r3_4h4N",
        "outputId": "2b5b6193-1df7-4ccf-cfaf-a7800d068b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 38814\n"
          ]
        }
      ],
      "source": [
        "# Simple tokenization (word-level, limited vocab)\n",
        "from collections import Counter\n",
        "\n",
        "MAX_SEQ_LEN = 50  # limit to reduce memory usage\n",
        "\n",
        "def build_vocab(pairs, min_freq=1):\n",
        "    counter = Counter()\n",
        "    for t1, t2, _ in pairs:\n",
        "        counter.update(t1.lower().split())\n",
        "        counter.update(t2.lower().split())\n",
        "    vocab = {w:i+2 for i, (w, freq) in enumerate(counter.items()) if freq >= min_freq}  # 0=PAD, 1=UNK\n",
        "    vocab['<PAD>'] = 0\n",
        "    vocab['<UNK>'] = 1\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(train_pairs)\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "def encode_text(text, vocab, max_len=MAX_SEQ_LEN):\n",
        "    tokens = [vocab.get(w, vocab['<UNK>']) for w in text.lower().split()]\n",
        "    tokens = tokens[:max_len]\n",
        "    tokens += [vocab['<PAD>']] * (max_len - len(tokens))\n",
        "    return tokens\n",
        "\n",
        "class ClausePairDataset(Dataset):\n",
        "    def __init__(self, pairs, vocab):\n",
        "        self.pairs = pairs\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        t1, t2, label = self.pairs[idx]\n",
        "        t1_enc = torch.tensor(encode_text(t1, self.vocab))\n",
        "        t2_enc = torch.tensor(encode_text(t2, self.vocab))\n",
        "        label = torch.tensor(label, dtype=torch.float)\n",
        "        return t1_enc, t2_enc, label\n",
        "\n",
        "# Dataloaders\n",
        "BATCH_SIZE = 32\n",
        "train_dataset = ClausePairDataset(train_pairs, vocab)\n",
        "val_dataset = ClausePairDataset(val_pairs, vocab)\n",
        "test_dataset = ClausePairDataset(test_pairs, vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1Uyv3Mo94h7Q"
      },
      "outputs": [],
      "source": [
        "class BiLSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*4, 1)  # concat two clause representations\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Encode first clause\n",
        "        e1 = self.embedding(x1)\n",
        "        _, (h1, _) = self.lstm(e1)\n",
        "        h1 = torch.cat((h1[-2], h1[-1]), dim=1)\n",
        "        # Encode second clause\n",
        "        e2 = self.embedding(x2)\n",
        "        _, (h2, _) = self.lstm(e2)\n",
        "        h2 = torch.cat((h2[-2], h2[-1]), dim=1)\n",
        "        # Concatenate and classify\n",
        "        out = torch.cat([h1, h2], dim=1)\n",
        "        out = torch.sigmoid(self.fc(out))\n",
        "        return out.squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EN0hcz5F4h97"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=64, num_filters=64, kernel_sizes=[3,4,5]):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(embed_dim, num_filters, k) for k in kernel_sizes])\n",
        "        self.fc = nn.Linear(num_filters*len(kernel_sizes)*2, 1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        def encode(x):\n",
        "            e = self.embedding(x).permute(0,2,1)  # batch x embed x seq\n",
        "            conv_out = [torch.relu(conv(e)) for conv in self.convs]\n",
        "            pool_out = [torch.max(c, dim=2)[0] for c in conv_out]\n",
        "            return torch.cat(pool_out, dim=1)\n",
        "        h1 = encode(x1)\n",
        "        h2 = encode(x2)\n",
        "        out = torch.cat([h1,h2], dim=1)\n",
        "        out = torch.sigmoid(self.fc(out))\n",
        "        return out.squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gNbgmmG74iA3"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=3, lr=1e-3):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for t1, t2, labels in train_loader:\n",
        "            t1, t2, labels = t1.to(device), t2.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(t1, t2)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        train_losses.append(total_loss/len(train_loader))\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for t1, t2, labels in val_loader:\n",
        "                t1, t2, labels = t1.to(device), t2.to(device), labels.to(device)\n",
        "                outputs = model(t1, t2)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "        val_losses.append(val_loss/len(val_loader))\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_losses[-1]:.4f}\")\n",
        "    return train_losses, val_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "c6pUe1184iDC"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred, y_prob = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for t1, t2, labels in loader:\n",
        "            t1, t2 = t1.to(device), t2.to(device)\n",
        "            outputs = model(t1, t2)\n",
        "            y_prob.extend(outputs.cpu().numpy())\n",
        "            y_pred.extend((outputs.cpu().numpy() >= 0.5).astype(int))\n",
        "            y_true.extend(labels.numpy())\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    roc_auc = roc_auc_score(y_true, y_prob)\n",
        "    return accuracy, precision, recall, f1, roc_auc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhkUSXH04iFL"
      },
      "outputs": [],
      "source": [
        "bilstm_model = BiLSTMClassifier(vocab_size)\n",
        "train_model(bilstm_model, train_loader, val_loader, epochs=3)\n",
        "metrics = evaluate_model(bilstm_model, test_loader)\n",
        "print(\"BiLSTM Test Metrics: Accuracy {:.4f}, Precision {:.4f}, Recall {:.4f}, F1 {:.4f}, ROC-AUC {:.4f}\".format(*metrics))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIrC4AWW4iId"
      },
      "outputs": [],
      "source": [
        "cnn_model = CNNClassifier(vocab_size)\n",
        "train_model(cnn_model, train_loader, val_loader, epochs=3)\n",
        "metrics = evaluate_model(cnn_model, test_loader)\n",
        "print(\"CNN Test Metrics: Accuracy {:.4f}, Precision {:.4f}, Recall {:.4f}, F1 {:.4f}, ROC-AUC {:.4f}\".format(*metrics))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7skxiITc4zvE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tXt6Bfk4zyq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pj0pkKU4z95"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
